{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "a310a4b312511ba3bf0f63f790f953779b1517e3a444f95ac5afc31fe4715d27"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import abspath\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf\n",
    "\n",
    "warehouse_location = abspath('spark-warehouse')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName(\"etl-posts-py\") \\\n",
    "        .config(\"spark.sql.warehouse.dir\", abspath('spark-warehouse')) \\\n",
    "        .enableHiveSupport() \\\n",
    "        .getOrCreate()\n",
    "\n",
    "    #print(SparkConf().getAll())\n",
    "\n",
    "    spark.sparkContext.setLogLevel(\"INFO\")\n",
    "\n",
    "    get_users = \"./data/user.json\"\n",
    "    get_posts = \"./data/posts.json\"\n",
    "    get_comments = \"./data/comments.json\"\n",
    "\n",
    "    dataframe_users = spark.read \\\n",
    "                    .format('json') \\\n",
    "                    .option('inferSchema', 'false') \\\n",
    "                    .option(\"multiline\",\"true\") \\\n",
    "                    .option('header', 'true') \\\n",
    "                    .json(get_users)\n",
    "    \n",
    "    dataframe_posts = spark.read \\\n",
    "                    .format('json') \\\n",
    "                    .option('inferSchema', 'false') \\\n",
    "                    .option(\"multiline\",\"true\") \\\n",
    "                    .option('header', 'true') \\\n",
    "                    .json(get_posts)\n",
    "\n",
    "    dataframe_comments = spark.read \\\n",
    "                    .format('json') \\\n",
    "                    .option('inferSchema', 'false') \\\n",
    "                    .option(\"multiline\",\"true\") \\\n",
    "                    .option('header', 'true') \\\n",
    "                    .json(get_comments)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DataFrame[summary: string, body: string, id: string, title: string, userId: string]"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "dataframe_posts.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe_users.createOrReplaceTempView('users')\n",
    "dataframe_posts.createOrReplaceTempView('posts')\n",
    "dataframe_comments.createOrReplaceTempView('comments')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_join = spark.sql(\n",
    "    '''\n",
    "        SELECT \n",
    "            u.name as author,\n",
    "            p.body as postagem,\n",
    "            c.body as comentarios,\n",
    "            c.email\n",
    "        FROM users as u\n",
    "        INNER JOIN posts as p\n",
    "        ON u.id = p.userId\n",
    "        INNER JOIN comments as c\n",
    "        ON p.id == c.postId\n",
    "    '''\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-------------+--------------------+--------------------+--------------------+\n|       author|            postagem|         comentarios|               email|\n+-------------+--------------------+--------------------+--------------------+\n|Leanne Graham|quia et suscipit\n...|harum non quasi e...|   Hayden@althea.biz|\n|Leanne Graham|quia et suscipit\n...|non et atque\nocca...|       Lew@alysha.tv|\n|Leanne Graham|quia et suscipit\n...|quia molestiae re...| Nikita@garfield.biz|\n|Leanne Graham|quia et suscipit\n...|est natus enim ni...|Jayne_Kuhic@sydne...|\n|Leanne Graham|quia et suscipit\n...|laudantium enim q...|  Eliseo@gardner.biz|\n|Leanne Graham|est rerum tempore...|voluptate iusto q...|Carmen_Keeling@ca...|\n|Leanne Graham|est rerum tempore...|sapiente assumend...|Meghan_Littel@ren...|\n|Leanne Graham|est rerum tempore...|ut voluptatem cor...|Mallory_Kunze@mar...|\n|Leanne Graham|est rerum tempore...|maiores sed dolor...|       Dallas@ole.me|\n|Leanne Graham|est rerum tempore...|doloribus at sed ...|Presley.Mueller@m...|\n|Leanne Graham|et iusto sed quo ...|nihil ut voluptat...|Maynard.Hodkiewic...|\n|Leanne Graham|et iusto sed quo ...|vel quae voluptas...|     Nathan@solon.io|\n|Leanne Graham|et iusto sed quo ...|fuga eos qui dolo...|    Kariane@jadyn.tv|\n|Leanne Graham|et iusto sed quo ...|expedita maiores ...|Oswald.Vandervort...|\n|Leanne Graham|et iusto sed quo ...|ut dolorum nostru...|Veronica_Goodwin@...|\n|Leanne Graham|ullam et saepe re...|qui harum consequ...|Mariana_Orn@prest...|\n|Leanne Graham|ullam et saepe re...|doloribus est ill...|Madelynn.Gorczany...|\n|Leanne Graham|ullam et saepe re...|veritatis volupta...|Vincenza_Klocko@a...|\n|Leanne Graham|ullam et saepe re...|consequatur neces...|Preston_Hudson@bl...|\n|Leanne Graham|ullam et saepe re...|iste ut laborum a...|Christine@ayana.info|\n+-------------+--------------------+--------------------+--------------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "data_join.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "source": [
    "data_join.count()"
   ]
  }
 ]
}